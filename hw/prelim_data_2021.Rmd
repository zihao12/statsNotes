---
title: "Data Analysis Prelimary Exam"
author: "Zihao Wang"
date: "2021-09-15"
output: pdf_document
editor_options:
  chunk_output_type: console
---


```{r message=FALSE, warning=FALSE, echo=FALSE}
rm(list = ls())
knitr::opts_chunk$set(message = FALSE, warning = -1)
library(dplyr)
library(extraDistr)
library(lmerTest)
library(ordinal)
library(ggplot2)
library(lattice)
library("GGally")
```

## Problem 1

### Paper summary

* The goal of this paper is to understand whether knowing that their report would be published affected the refereesâ€™ willingness to review, the type of recommendations, the turn-around time and the tone of the report.

* The authors use observational data from the pilot prgram of the Publication of Peer Review reports. The data included 62,790 observations linked to 9220 submissions and 18,525 completed reviews from January 2010 to November 2017, including the pilot and non-pilot period. For gender & academic status of referee and sentiment of review, they used data mining and sentiment analysis. Note that these preprocessing steps may introduce bias, and they sometimes leave many "uncertain" cases. 

* They fit mixed effect linear & GLM models for various responses. They confirmed their findings by fitting 3 other models using both the pilot journal data and a control dataset of other journals, which they claim are selected to match the five pilot journals. The results are consistent with original models. 

* Their main conclusion is: open peer review does not compromise the various aspects of review process: willingness to review, recommendations, or turn-around times. 

### Statistical issues

* Some covariates, and even responses (polarity) are generated by some imputation steps. We do not know how much bias and uncertaintythose steps introduce , and of course the model does not account for that. Also these steps have many uncertain cases (for example gender has 20% being uncertain) and they treat them as `other` in the model. If they are "uncertain" for some reason, it might affect the inference. 

* As the data is observational, it has confounders. For example pilot program only starts after certain time. So `year` may affect those responses directly, or affect them through open review. It's hard to distinguish these two effects. That they only add a linear effect of year may not be adequate . 

* As the authors point out in the end, the same referee could have reviewed many manuscripts but they do not have the data for that. Therefore the individual observations can be dependent. This can make the standard errors invalid. 




```{r echo=FALSE}
# Data upload and preparation
round1 <- read.csv("../prelim_21/RevData.csv")
round1$id <- as.character(round1$id)
round1$journal <- factor(round1$journal, labels=c("Journal 1", "Journal 2", "Journal 3", "Journal 4", "Journal 5"))
round1$open.review <- factor(round1$open.review, labels=c("No", "Yes"))
round1$review.complete <- factor(round1$review.complete, labels=c("No", "Yes"))
round1$name.published <- factor(round1$name.published, labels=c("No", "Yes"))
round1$recommendation <- factor(round1$recommendation, labels=c("Reject", "Major revisions", "Minor revisions", "Accept"))
round1$accepted <- factor(round1$accepted, labels=c("No", "Yes"))
round1$reviewer.status <- factor(round1$reviewer.status, labels=c("Professor", "Other", "Dr."))
round1$gender <- factor(round1$gender, labels=c("Female", "Male", "Uncertain"))
round1$invitation.date = as.Date(round1$invitation.date)
# summary(round1)
```

## Problem 2
```{r echo=FALSE}
# J = "Journal 1"
# data_sub = round1[round1$journal == J, ]
by_date = group_by(round1, journal, invitation.date)
prop <- summarise(by_date,
                   proportion = mean(accepted == "Yes"))
prop_by_journal <- group_by(prop, journal)
prop_by_journal$invitation.date = as.Date(prop_by_journal$invitation.date)
# tail(unique(prop_by_journal$invitation.date), 20)
## rm last 6 months
prop_by_journal = prop_by_journal[prop_by_journal$invitation.date < "2017-05-01", ]
# tail(unique(prop_by_journal$invitation.date), 20)

my_cols = c("purple", "blue", "cyan", "green", "yellow")

i = 1
J = sprintf("Journal %d", i)
data_sub = prop_by_journal[prop_by_journal$journal == J, ]
attach(data_sub)
prop_sm = predict(loess(proportion ~ as.numeric(invitation.date), data = data_sub))

plot(invitation.date, proportion, 
     type = "l", col = my_cols[i],
     ylim = c(0, 0.8), xlab = "Date", ylab = "proportion of accpectance",
     main = toString(paste("J",1:5, my_cols)),
     cex.main=0.8)
lines(invitation.date, prop_sm, col = my_cols[i])
detach(data_sub)

for(i in 2:5){
  J = sprintf("Journal %d", i)
  data_sub = prop_by_journal[prop_by_journal$journal == J, ]
  attach(data_sub)
  prop_sm = predict(loess(proportion ~ as.numeric(invitation.date), data = data_sub))
  
  lines(invitation.date, proportion, type = "l", col = my_cols[i])
  lines(invitation.date, prop_sm, col = my_cols[i])
  detach(data_sub)
}
```

Visual assessmen of smoothing: the chosen level of smoothing reflects the long term trend (and this supports the claim author makes) but neglects more local variabilities. There seems to be some "seasonal" fluctuations that are flattend out as noise. 

## Problem 3
Yes, we can find a regression model that has the same assumption as fitting the 9 subgroups separately. 

### Analytical justification


For sample $i$, let $s_i$ denote the status and $g_i$ denote gender, both taking values from $\{1, 2, 3\}$; let $o_i$ denote the open review, taking values from $\{1, 2\}$. If we fit the 9 subgroups separately, we can write the model as:
\begin{align}
  & \text{logit}(\pi_i) = \alpha^0_{s_i, g_i} + \alpha^1_{s_i, g_i} 1_{o_i = 2}
\end{align}

We can fit the models above in one model:
\begin{align}
   \text{logit}(\pi_i) = & \beta^0 + \sum_{s \geq 2} \beta^{0, S}_s 1_{s_i = s} + \sum_{g \geq 2} \beta^{0, G}_g 1_{g_i = s} + \sum_{s \geq 2, g \geq 2} \beta^{0, SG}_s 1_{s_i = s, g_i = g}\\
   & + (\beta^1 + \sum_{s \geq 2} \beta^{1, S}_s 1_{s_i = s} + \sum_{g \geq 2} \beta^{1, G}_g 1_{g_i = s} + \sum_{s \geq 2, g \geq 2} \beta^{1, SG}_s 1_{s_i = s, g_i = g}) 1_{o_i = 2}
\end{align}

Then to show that the two models have the same assumptions, it suffices to show, for any $\alpha^{k}_{sg}$ we can express $\pi_i$ equivalently with a set of $\beta$'s, and vice versa. In fact, it's easy to see

\begin{align}
  & \alpha^0_{11} = \beta^0\\
  & \alpha^0_{s1} = \beta^0 + \beta^{0, S}_s, \forall s \geq 2\\
  & \alpha^0_{1g} = \beta^0 + \beta^{0, G}_g, \forall g \geq 2\\
  & \alpha^0_{sg} = \beta^0 + \beta^{0, S}_s + \beta^{0, G}_g + \beta^{0, SG}_{sg}, \forall s \geq 2, g \geq 2
\end{align}

Similarly for intercept terms
\begin{align}
  & \alpha^1_{11} = \beta^1\\
  & \alpha^1_{s1} = \beta^1 + \beta^{1, S}_s, \forall s \geq 2\\
  & \alpha^1_{1g} = \beta^1 + \beta^{1, G}_g, \forall g \geq 2\\
  & \alpha^1_{sg} = \beta^1 + \beta^{1, S}_s + \beta^{1, G}_g + \beta^{1, SG}_{sg}, \forall s \geq 2, g \geq 2
\end{align}
If we flatten the $\alpha$'s and $\beta$'s and express as vectors, we can find $\alpha = L \beta$ where $L$ is full rank (with some ordering it's upper triangular with diagonal elements all 1). Once we see the linear 1-1 correspondence between $\alpha, \beta$, we can easily see the ML estimate should be the same. Also since $\hat{\alpha}, \hat{\beta}$ are both asymptotically normal, from one covariance matrix we can recover the other (if $\hat{\beta} - \beta \longrightarrow N(0, \Sigma)$, we have $\hat{\alpha} - \alpha \longrightarrow N(0, L\Sigma L^T)$). Thus after doing some transformation we can recover the same SE and thus the same confidence interval. 

### Numerical check

* First, I check to see that the $\hat{\pi}_i$'s are the same for the two models (I compared two subgroups). The max difference is due to numerical rounding. 
```{r echo=FALSE}
## compare \pi_i's
acceptance <- glm(accepted ~ open.review * gender * reviewer.status, 
                  family=binomial, data=round1)

acceptance_sub <- glm(accepted ~ open.review , 
                  family=binomial, data=round1, subset= gender=="Female" & reviewer.status == "Professor")
p1 = predict.glm(acceptance_sub, type = "response")

acceptance_sub2 <- glm(accepted ~ open.review , 
                  family=binomial, data=round1, subset= gender=="Male" & reviewer.status == "Professor")
p2 = predict.glm(acceptance_sub2, type = "response")

p0 = predict.glm(acceptance, type = "response")[which(round1$gender == "Female" & round1$reviewer.status == "Professor")]
d1 = max(abs(p1 - p0))
p0 = predict.glm(acceptance, type = "response")[which(round1$gender == "Male" & round1$reviewer.status == "Professor")]
d2 = max(abs(p2 - p0))
c(d1, d2)
```

* Next, I should how to compute $\alpha$'s SE from $\beta$ using the examples below. The easiest one is $\alpha^1_{11}$, which is the same as baseline for $\beta$'s.  

```{r echo=FALSE}
summary(acceptance_sub)$coefficients["open.reviewYes",1:2]
summary(acceptance)$coefficients["open.reviewYes",1:2]
```

* Then to compute $\alpha^{1}_{1g}$, I find $v$ s.t. $\alpha^{1}_{1g} = v^T \beta$ and then use it to compute the mean and SE. Comparing with the results from separate model, we can see they are the same. 
```{r echo=FALSE}
betas = coefficients(acceptance)
M = vcov(acceptance)
v = replicate(18, 0)
v[2] = v[7] = 1
summary(acceptance_sub2)$coefficients["open.reviewYes",1:2]
c(t(v) %*% betas, sqrt(t(v) %*% M %*% v))
```

Now it's obvious how to find estimates and SE from for other $\alpha$'s from separate models using $\beta$ from one model. And similarly other way round. 



## Problem 4
It's not possible. Separately fitting 9 models, each subgroup has their own cutoff points (for cumulative-logit link), whereas fitting them in one model will give us one set of cutoff points for all the sugroups. Therefore can't find the same model. 


## Problem 5

```{r echo=FALSE}
lrt <- function(fit1, fit2){
  1 - pchisq(2 * as.numeric(logLik(fit2) - logLik(fit1)), df = df.residual(fit1) - df.residual(fit2))
}

data_sub = subset(round1, journal %in% paste("Journal", c(1,3,5)) & invitation.date < "2014-11-01" & review.complete=="Yes")
data_sub$journal = factor(data_sub$journal) ## make the levels right
data_sub$year = factor(data_sub$year)
data_sub = na.omit(data_sub)

# ggpairs(data_sub[,c("review.time","journal", "year", "open.review", "reviewer.status")])
# bwplot(year ~ review.time, data = data_sub)
```

* First, I did pairwise barplot (not shown here) and find `review.time` differ across `journal`, `reviewer.status` and `open.review`. The `year` differences seem small but I will still include it as a factor as it is a confounder. The differences across `gender` is not obvious. 

* To start, I will use `journal` and `id` as random effect, and `year`, `reviewer.status` and `open.review` as fixed, and include the interaction between `reviewer.status` and `open.review`. (Note the high correlation for observations of one journal or one paper, 0.18 and 0.32 respectively).
```{r}
time1 <- lmer(review.time ~ open.review  + reviewer.status + factor(year) + open.review:reviewer.status + 
                (1 | id) + (1 | journal), data=data_sub, REML = FALSE)
plot(time1)

# time2 <- lmer(review.time ~ open.review  + reviewer.status + factor(year) + 
#                 (1 | id) + (1 | journal), data=data_sub, REML = FALSE)
# time3 <- lmer(review.time ~ reviewer.status + factor(year) + 
#                 (1 | id) + (1 | journal), data=data_sub, REML = FALSE)
# 
# lrt(time2, time1)
# lrt(time3, time1)
```

* However, the residual plot is problematic: residual variance increases with fitted values. Since the response `review.time` is counts of days, I will try with Poisson GLM

```{r}
time2 <- glmer(review.time ~ open.review  + reviewer.status + factor(year) + open.review:reviewer.status +
                (1 | id) + (1 | journal), data=data_sub, family = poisson(link = "log"))
plot(time2)

time3 <- glmer(review.time ~ open.review  + reviewer.status + factor(year) +
                (1 | id) + (1 | journal), data=data_sub, family = poisson(link = "log"))

time4 <- glmer(review.time ~ reviewer.status + factor(year) +
                (1 | id) + (1 | journal), data=data_sub, family = poisson(link = "log"))

lrt(time3, time2)
lrt(time4, time2)
```



## Problem 6

First, let's take a look at the average acceptance probability for the five journals (`n` is total number of invitations, `s` is total number of acceptances and `prop` is the proportion of the invitations accepted)
```{r echo=FALSE}
by_id = group_by(round1, journal, id)
s_n <- summarise(by_id,
                 n = n(),
                 s = sum(accepted == "Yes"),
                 mle = s/n)

s_n_journal <- group_by(s_n, journal)
s_n_journal <- summarise(s_n_journal,
                         n = sum(n),
                         s = sum(s),
                         prop = s/n)
s_n_journal
```

Next, I want to assess the variabilities in acceptance probability ($p_j$) across papers in each journal. Directly relying on ML estimate of $p_j$ is unreliable as most papers have small number of invited reviewers ($n_j$ below). We can improve estimate of $p_j$ using Empirical Bayes approach (I didn't use the full bayesian approach because there are enough samples to estimate the prior in EB, and EB's computation is easier). I model $p_j$ with a binomial model for simplicity, and put a beta prior and estimate it using marginal MLE. Note for numerical stability I fix the prior mean to be at the average acceptance probability for each journal (`prop` above). 


More specifically, the model is 
\begin{align}
 & p_j \sim \text{Beta}(a, b)\\
 & s_j \sim \text{Bin}(n_j, p_j)
\end{align}
The posterior is
\begin{align}
 & p_j |s_j, n_j, a, b  \sim \text{Beta}(a + s_j, b + n_j - s_j)\\
\end{align}



```{r echo=FALSE}
obj <- function(par, s, n, fix_mean){
  a = (par[1])**2
  b = a * (1/fix_mean - 1)
  nll = -sum(dbbinom(x = s, size = n, alpha = a, beta = b, log = TRUE))
  return(nll)
}

mle <- function(s, n, fix_mean){
  fit = optimize(f = obj, c(0, 10000), s = s, n = n, fix_mean = fix_mean)
  val = fit$objective
  a = fit$minimum**2
  b = a * (1/fix_mean - 1)
  return(c(a, b, val))
}

is_significant <- function(ab, lower.tail, q, p = 0.95){
  pbeta(q = q, shape1 = ab[1], shape2 = ab[2], lower.tail = lower.tail) > p
}


EB <- function(s, n){
  ## fix prior at mean
  fix_mean = sum(s)/sum(n)
  fit = mle(s, n, fix_mean)
  a = fit[1]
  b = fit[2]
  alpha = a + s
  beta = b + n - s
  posterior_mean = alpha/(alpha + beta)
  posterior_var = alpha*beta/((alpha + beta)^2 * (alpha + beta + 1))
  pH = mean(apply(cbind(alpha, beta), 1,
                 is_significant, lower.tail = FALSE, q = fix_mean))
  pL = mean(apply(cbind(alpha, beta), 1,
                 is_significant, lower.tail = TRUE, q = fix_mean))
  return(list(a = a, b = b, alpha = alpha, beta = beta,
              sample_mean = fix_mean,
              posterior_mean = posterior_mean,
              posterior_var = posterior_var,
              pH = pH, pL = pL,
              nll = fit[3]))
}
```



After fitting model, I assess the variability of $p_j$ in each journal by: (1) how many papers have significantly different $p_j$ (that their 90% credible interval do not contain the average probability for that journal); (2) check the shrinkage effect of EB (stronger shrinkage towards the mean indicates less ovrall variation); (3) look at the distrbution of posterior mean of $p_j$. 
```{r echo=FALSE}
fitted = list()
for(i in 1:5){
  J = sprintf("Journal %d", i)
  data_sub = subset(s_n, journal == J)
  fitted[[i]] = EB(data_sub$s, data_sub$n)
}

out <- c()
for(i in 1:5){
  mod = fitted[[i]]
  a_b = mod$a + mod$b
  tmp <- c(mod$a, mod$b, mod$pH, mod$pL)
  out = rbind(out, tmp)
}

out = as.data.frame(out)
rownames(out) <- paste("J", 1:5)
colnames(out) <- c("a", "b", "pH","pL")
round(out, 5)
```


```{r fig.width=30, fig.height=70, fig.align='center', echo=FALSE}
par(mfrow = c(5, 2), mar=c(3,3,3,3))
for(i in 1:5){
  J = sprintf("Journal %d", i)
  data_sub = subset(s_n, journal == J)
  mod = fitted[[i]]
  plot(data_sub$mle, mod$posterior_mean,
       xlim = c(0,1), ylim = c(0,1),
       xlab = "mle", ylab = "posterior mean", main = J)
  abline(h = mod$sample_mean, col = "blue")
  hist(mod$posterior_mean, probability = TRUE, breaks = 50, 
       xlab = "proportion (posterior mean)", main = J,  xlim = c(0,1))
  abline(v = mod$sample_mean, col = "blue")
}
```

The plots above are MLE vs posterior mean and histogram of posterior mean ((blue lines are both prior mean)). 

* The histogram of posterior mean for $p_j$ shows Journal 5 has the largest variance; Journal 1 & 3 has a heavy left tail. 
* Compared against MLE, we can see a strong shrinkage effect from the EB, but Journal 5 is shrunk less, which is consistent with the posterior mean distribution. Similarly, there are some small $p_j$ not shrunk heavily in journal 1 & 3, corresponding to the left tail in histogram (these samples have large $n_j$). On the contrary, Journal 4 is shrunk very heavily, indictating less variability. 
* Finally, Journal 5 & 2 have the most significantly different $p_j$'s (4.8% and 3.7%, from the `pH` and `pL` in the table above). The significant papers mostly have significantly lower acceptance probabilities.  


